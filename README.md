# IPL Data Analytics and Match Winner Prediction

This project performs an in-depth analysis of the Indian Premier League (IPL) dataset and uses various machine learning models to predict match winners.

## üåü Features

*   **Data Analysis:** Explores the IPL dataset to uncover trends and insights.
*   **Data Visualization:** Creates various plots to visualize data, such as:
    *   Top 5 teams by wins
    *   Win percentage of top 5 teams
    *   Toss decision distribution
    *   Impact of toss on match outcome
    *   Top 5 players of the match
    *   Top 5 cities by matches hosted
    *   Top 10 umpires
    *   Win margin distribution
*   **Machine Learning Models:** Implements and evaluates the following models for match winner prediction:
    *   Logistic Regression
    *   Random Forest
    *   Gradient Boosting
    *   K-Nearest Neighbors (KNN)
    *   AdaBoost
    *   CatBoost
    *   Decision Tree
    *   SVM
    *   XGBoost
*   **Model Evaluation:** Evaluates the performance of each model using various metrics, including:
    *   Accuracy
    *   Precision
    *   Recall
    *   F1 Score
    *   Confusion Matrix

## üì¶ Installation

1.  Clone the repository:
    ```bash
    git clone https://github.com/tarunsahu90/IPL-DATA-ANALYTICS-WITH-USING-PYTHON--Machine-Leaming-.git
    ```
2.  Install the required libraries:
    ```bash
    pip install pandas numpy matplotlib seaborn scikit-learn
    ```

## üöÄ Usage

1.  Open the `programms.ipynb` file in a Jupyter Notebook environment.
2.  Run the cells to see the data analysis, visualizations, and model predictions.

## üìä Data

The project uses two CSV files:

*   `matches.csv`: Contains information about each match, such as the teams, toss winner, and venue.
*   `deliveries.csv`: Contains ball-by-ball data for each match.

## ü§ñ Models Used

The following machine learning models are used in this project:

*   **Logistic Regression:** A linear model for classification.
*   **Random Forest:** An ensemble learning method that uses multiple decision trees.
*   **Gradient Boosting:** An ensemble learning method that builds trees one at a time, where each new tree helps to correct errors made by previously trained trees.
*   **K-Nearest Neighbors (KNN):** A non-parametric method used for classification and regression.
*   **AdaBoost:** An ensemble learning method that combines multiple weak learners to create a strong learner.
*   **CatBoost:** A gradient boosting library that is well-suited for working with categorical data.
*   **Decision Tree:** A tree-like model of decisions and their possible consequences.
*   **SVM:** A supervised machine learning model that uses classification algorithms for two-group classification problems.
*   **XGBoost:** An optimized distributed gradient boosting library designed to be highly efficient, flexible and portable.


## üìà Results

The performance of the models is evaluated using various metrics. The results are presented in the notebook.

## ü§ù Contributing

Contributions are welcome! Please feel free to submit a pull request.

## üìÑ License

This project is licensed under the MIT License.